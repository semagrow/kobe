{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to KOBE KOBE is a benchmarking system that leverages Docker and Kubernetes in order to reproduce experiments of federated query processing in a collections of data sources. Overview In the SPARQL query processing community, as well as in the wider databases community, benchmark reproducibility is based on releasing datasets and query workloads. However, this paradigm breaks down for federated query processors, as these systems do not manage the data they serve to their clients but provide a data-integration abstraction over the actual query processors that are in direct contact with the data. The KOBE benchmarking engine is a system that aims to provide a generic platform to perform benchmarking and experimentation that can be reproducible in different environments. It was designed with the following objectives in mind: to allow for benchmark and experiment specifications to be reproduced in different environments and be able to produce comparable and reliable results; to ease the deployment of complex benchmarking experiments by automating the tedious tasks of initialization and execution.","title":"Home"},{"location":"#welcome-to-kobe","text":"KOBE is a benchmarking system that leverages Docker and Kubernetes in order to reproduce experiments of federated query processing in a collections of data sources.","title":"Welcome to KOBE"},{"location":"#overview","text":"In the SPARQL query processing community, as well as in the wider databases community, benchmark reproducibility is based on releasing datasets and query workloads. However, this paradigm breaks down for federated query processors, as these systems do not manage the data they serve to their clients but provide a data-integration abstraction over the actual query processors that are in direct contact with the data. The KOBE benchmarking engine is a system that aims to provide a generic platform to perform benchmarking and experimentation that can be reproducible in different environments. It was designed with the following objectives in mind: to allow for benchmark and experiment specifications to be reproduced in different environments and be able to produce comparable and reliable results; to ease the deployment of complex benchmarking experiments by automating the tedious tasks of initialization and execution.","title":"Overview"},{"location":"extend/add_dataset_server/","text":"Add a new dataset server This walkthrough illustrates the steps required from the implementor of a dataset server in order to create a DatasetTemplate specification. In KOBE, a dataset template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Dataset templates are used in the Benchmark specifications in order to define the dataset server of the federated endpoints. Prerequisites In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the dataset server (e.g., https://hub.docker.com/r/openlink/virtuoso-opensource-7). Step 1. Prepare your Docker images The first step is to provide a set of one or more Docker images that downloads the dataset, loads the data, and starts the dataset server. Even though all this functionality can be provided with a single image, we suggest to split the various tasks into three separate images. More specifically: A docker image that downloads a RDF dump from a known URL (found in the variable $DATASET_URL ) and extracts its contents in the directory /kobe/dataset/$DATASET_NAME/dump . A docker image that loads the downloaded dump (already present in the directory /kobe/dataset/$DATASET_NAME/dump ) into the dataset server. Optionally, it can back-up the contents of the database in some directory inside /kobe/dataset/$DATASET_NAME such that the loading process to be executed only once. A docker image that starts the dataset server and exposes its SPARQL endpoint. The environment variables are initialized by the KOBE operator according to the specification of the benchmark to be executed. Moreover, the shared volumes are managed through the KOBE operator too (ref. here for details about the shared storage of KOBE). In the benchmark walkthrough, we suggest that the dataset dumps should follow a specific format. Therefore, feel free to use semagrow/url-donwnloader (source code here ) as your first image. However, if you optionally want your template to support more dataset dump formats, you can implement your own url downloader. As an example, we present the images for two dataset servers (namely Virtuoso and Strabon). Regarding the Virtuoso RDF store, we use the images semagrow/url-donwnloader , semagrow/virtuoso-init (source code here ), and semagrow/virtuoso-main (source code here ). We use the shared storage of KOBE, to keep a backup of the /database directory of Virtuoso, which is used to keep all the files used by the database. The last two images are built upon openlink/virtuoso-opensource-7 . Regarding the Strabon geospatial RDF store, we use the images semagrow/url-downloader , semagrow/strabon-init (source code here ), and semagrow/strabon-main (source code here ). We use the shared storage of KOBE, to keep a backup of the PostGIS database (directory /var/lib/postgresql/9.4/main ) which is where the data are kept inside Strabon . The last two images are built using the docker file of KR-suite (see http://github.com/GiorgosMandi/KR-Suite-docker)`. Step 2. Prepare your YAML file Once you have prepared the docker images, creating the dataset template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Virtuoso): apiVersion : kobe.semagrow.org/v1alpha1 kind : DatasetTemplate metadata : # Each dataset template can be uniquely identified by its name. name : virtuosotemplate spec : initContainers : # here you put the first two images (that is the images for initializing # your server in the order you want to be executed). - name : initcontainer0 image : semagrow/url-downloader - name : initcontainer1 image : semagrow/virtuoso-init containers : # here you put the last image (that is the image for serving the data) - name : maincontainer image : semagrow/virtuoso-main ports : - containerPort : 8890 # port to listen for queries port : 8890 # port to listen for queries path : /sparql # path to listen for queries The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8890/sparql , hence the port and the path to listen for queries are 8890 and /sparql respectively. Examples We have already prepared several dataset template specifications to experiment with: dataset-virtuoso dataset-strabon Note We plan to define more dataset template specifications in the future. We place all dataset template specifications in the examples/ directory under a subdirectory with the prefix dataset-* .","title":"Add dataset servers"},{"location":"extend/add_dataset_server/#add-a-new-dataset-server","text":"This walkthrough illustrates the steps required from the implementor of a dataset server in order to create a DatasetTemplate specification. In KOBE, a dataset template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Dataset templates are used in the Benchmark specifications in order to define the dataset server of the federated endpoints.","title":"Add a new dataset server"},{"location":"extend/add_dataset_server/#prerequisites","text":"In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the dataset server (e.g., https://hub.docker.com/r/openlink/virtuoso-opensource-7).","title":"Prerequisites"},{"location":"extend/add_dataset_server/#step-1-prepare-your-docker-images","text":"The first step is to provide a set of one or more Docker images that downloads the dataset, loads the data, and starts the dataset server. Even though all this functionality can be provided with a single image, we suggest to split the various tasks into three separate images. More specifically: A docker image that downloads a RDF dump from a known URL (found in the variable $DATASET_URL ) and extracts its contents in the directory /kobe/dataset/$DATASET_NAME/dump . A docker image that loads the downloaded dump (already present in the directory /kobe/dataset/$DATASET_NAME/dump ) into the dataset server. Optionally, it can back-up the contents of the database in some directory inside /kobe/dataset/$DATASET_NAME such that the loading process to be executed only once. A docker image that starts the dataset server and exposes its SPARQL endpoint. The environment variables are initialized by the KOBE operator according to the specification of the benchmark to be executed. Moreover, the shared volumes are managed through the KOBE operator too (ref. here for details about the shared storage of KOBE). In the benchmark walkthrough, we suggest that the dataset dumps should follow a specific format. Therefore, feel free to use semagrow/url-donwnloader (source code here ) as your first image. However, if you optionally want your template to support more dataset dump formats, you can implement your own url downloader. As an example, we present the images for two dataset servers (namely Virtuoso and Strabon). Regarding the Virtuoso RDF store, we use the images semagrow/url-donwnloader , semagrow/virtuoso-init (source code here ), and semagrow/virtuoso-main (source code here ). We use the shared storage of KOBE, to keep a backup of the /database directory of Virtuoso, which is used to keep all the files used by the database. The last two images are built upon openlink/virtuoso-opensource-7 . Regarding the Strabon geospatial RDF store, we use the images semagrow/url-downloader , semagrow/strabon-init (source code here ), and semagrow/strabon-main (source code here ). We use the shared storage of KOBE, to keep a backup of the PostGIS database (directory /var/lib/postgresql/9.4/main ) which is where the data are kept inside Strabon . The last two images are built using the docker file of KR-suite (see http://github.com/GiorgosMandi/KR-Suite-docker)`.","title":"Step 1. Prepare your Docker images"},{"location":"extend/add_dataset_server/#step-2-prepare-your-yaml-file","text":"Once you have prepared the docker images, creating the dataset template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Virtuoso): apiVersion : kobe.semagrow.org/v1alpha1 kind : DatasetTemplate metadata : # Each dataset template can be uniquely identified by its name. name : virtuosotemplate spec : initContainers : # here you put the first two images (that is the images for initializing # your server in the order you want to be executed). - name : initcontainer0 image : semagrow/url-downloader - name : initcontainer1 image : semagrow/virtuoso-init containers : # here you put the last image (that is the image for serving the data) - name : maincontainer image : semagrow/virtuoso-main ports : - containerPort : 8890 # port to listen for queries port : 8890 # port to listen for queries path : /sparql # path to listen for queries The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8890/sparql , hence the port and the path to listen for queries are 8890 and /sparql respectively.","title":"Step 2. Prepare your YAML file"},{"location":"extend/add_dataset_server/#examples","text":"We have already prepared several dataset template specifications to experiment with: dataset-virtuoso dataset-strabon Note We plan to define more dataset template specifications in the future. We place all dataset template specifications in the examples/ directory under a subdirectory with the prefix dataset-* .","title":"Examples"},{"location":"extend/add_evaluator/","text":"Add a new evaluator This walkthrough illustrates the steps required from the experimenter in order to implement a custom evaluator for KOBE. In KOBE, evaluators is defined using a single Docker image, and are used in the Experiment specifications in order to apply a query load to the SPARQL endpoint of a federation engine. Step 1. Prepare your Docker image Given a set of query strings and the SPARQL endpoint of the federator, the evaluator should issue each query of the queryset one or more times to the endpoint. For each query, it should calculate the number of the returned results and the time passed to receive the results, and export this information via a log message. KOBE provides the following input to the evaluator: All queries of the experiment are stored in the /queries directory. For each query, there exists a text file that contains the SPARQL query. The name of the file is the is used to uniquely identify each query of the experiment. The SPARQL endpoint of the federation engine appears in the $ENDPOINT environment variable. The environment variable $EVAL_RUNS specify how many times each query of the experiment should be executed. The name of the experiment to be executed appears in the $EXPERIMENT environment variable. This information is mainly used for logging purposes. Given this information, the evaluator can proceed in the evaluation of the queryset with its strategy of choice. However, the evaluator for each query should do the following: Before executing each query, attach a SPARQL comment in the first line of the query. The comment should look like this: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ where * <experiment> is the name of the experiment, obtained by $EXPERIMENT . * <date> is the date and time that this specific experiment started - same for all queries of this experiment. * <query> is the filename of the query that is going to be executed. * <run> which is a number between 1 and $EVAL_RUNS and is used to identify the run of this query. After executing each query, output a log message of the following form: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Query Evaluation Time: (?<evaluation_time>[0-9]+) - Results: (?<results>[0-9]+)$ where * <experiment> , <date> , <query> , <run> are defined as previously. * <evaluation_time> is the time passed in ms to get the full result set of the query. * <results> is number of returned results of the query. Note This guide describes the steps for the standard evaluator metrics in KOBE. If you want to add more metrics from the side of the evaluator, please refer to this guide . Example As an example, we have prepared semagrow/kobe-sequential-evaluator , which evaluates the queries in a sequential order, which is a slightly modified version of that of FedBench . ./evaluator We plan to implement more evaluators in the future.","title":"Add query evaluators"},{"location":"extend/add_evaluator/#add-a-new-evaluator","text":"This walkthrough illustrates the steps required from the experimenter in order to implement a custom evaluator for KOBE. In KOBE, evaluators is defined using a single Docker image, and are used in the Experiment specifications in order to apply a query load to the SPARQL endpoint of a federation engine.","title":"Add a new evaluator"},{"location":"extend/add_evaluator/#step-1-prepare-your-docker-image","text":"Given a set of query strings and the SPARQL endpoint of the federator, the evaluator should issue each query of the queryset one or more times to the endpoint. For each query, it should calculate the number of the returned results and the time passed to receive the results, and export this information via a log message. KOBE provides the following input to the evaluator: All queries of the experiment are stored in the /queries directory. For each query, there exists a text file that contains the SPARQL query. The name of the file is the is used to uniquely identify each query of the experiment. The SPARQL endpoint of the federation engine appears in the $ENDPOINT environment variable. The environment variable $EVAL_RUNS specify how many times each query of the experiment should be executed. The name of the experiment to be executed appears in the $EXPERIMENT environment variable. This information is mainly used for logging purposes. Given this information, the evaluator can proceed in the evaluation of the queryset with its strategy of choice. However, the evaluator for each query should do the following: Before executing each query, attach a SPARQL comment in the first line of the query. The comment should look like this: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ where * <experiment> is the name of the experiment, obtained by $EXPERIMENT . * <date> is the date and time that this specific experiment started - same for all queries of this experiment. * <query> is the filename of the query that is going to be executed. * <run> which is a number between 1 and $EVAL_RUNS and is used to identify the run of this query. After executing each query, output a log message of the following form: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Query Evaluation Time: (?<evaluation_time>[0-9]+) - Results: (?<results>[0-9]+)$ where * <experiment> , <date> , <query> , <run> are defined as previously. * <evaluation_time> is the time passed in ms to get the full result set of the query. * <results> is number of returned results of the query. Note This guide describes the steps for the standard evaluator metrics in KOBE. If you want to add more metrics from the side of the evaluator, please refer to this guide .","title":"Step 1. Prepare your Docker image"},{"location":"extend/add_evaluator/#example","text":"As an example, we have prepared semagrow/kobe-sequential-evaluator , which evaluates the queries in a sequential order, which is a slightly modified version of that of FedBench . ./evaluator We plan to implement more evaluators in the future.","title":"Example"},{"location":"extend/add_federator/","text":"Add a new federator This walkthrough illustrates the steps required from the implementor of a federation engine in order to create a FederatorTemplate specification. In KOBE, a federator template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Federator templates are used in the Experiment specifications in order to define the federation engine to be benchmarked. Prerequisites In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the federation engine (e.g., https://hub.docker.com/r/semagrow/semagrow/). Moreover, you should have a piece of software that automatically constructs the configuration required for your federator to operate (e.g., https://github.com/semagrow/sevod-scraper). Step 1. Prepare your Docker images Usually, a federation engine requires some configuration files that depend on the federated endpoints (e.g., the URLs of the federated SPARQL endpoints). Thus, apart from the Docker image with the SPARQL endpoint of the federation engine, you should provide a docker image that constructs any desired configuration for each of the source endpoints, and a docker image that initializes the federator that possibly takes into account the configuration files of the source endpoints. More specifically, prepare the following images: A docker image that constructs a configuration file for a source endpoint and places it in an output directory of your choice. Assume that the source endpoint and the dataset name are available in the environment variables $DATASET_NAME and $DATASET_URL respectively, and that the dump file of the dataset is present in an input directory of your choice. A docker image that constructs a configuration file for the federation engine and places it in an output directory of your choice. Assume that all the configuration files produced in the previous step are present in an input directory of your choice. A docker image that starts the federation engine and exposes its SPARQL endpoint. The environment variables are initialized by the KOBE operator according to the specification of the benchmark to be executed. As an example, we present the images for two federation engines (namely Fedx and Semagrow). Regarding the Semagrow federation engine, we use the images semagrow/semagrow-init (source code here ), semagrow/semagrow-init-all , (source code here ), and semagrow/semagrow (see here ). The first image uses the sevod-scraper tool to create a ttl metadata file from the dump file, and the second image concatenates all metadata files of each of the source endpoints into a single metadata file. Regarding the fedx federation engine, we use the images semagrow/fedx-init (source code here ), semagrow/fedx-init-all , (source code here ), and semagrow/fedx-server (source code here ). Fedx is known for not using any dataset statistics, but it uses only a ttl file that contains only the SPARQL endpoints of the federation. The first image creates a ttl file that defines the SPARQL endpoint of each dataset and the second image concatenates all ttl files of each source endpoints into a single configuration file. Step 2. Prepare your YAML file Once you have prepared the docker images, creating the federator template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Semagrow): apiVersion : kobe.semagrow.org/v1alpha1 kind : FederatorTemplate metadata : # Each federator template can be uniquely identified by its name. name : semagrowtemplate spec : containers : # here you put the last image (that is the image for the # SPARQL endpoint of the federation engine) - name : maincontainer image : semagrow/semagrow ports : - containerPort : 8080 # port to listen for queries port : 8080 # port to listen for queries path : /SemaGrow/sparql # path to listen for queries fedConfDir : /etc/default/semagrow # where the federator expects to find its configuration # federator configuration step 1 (for each dataset): confFromFileImage : semagrow/semagrow-init # first docker image inputDumpDir : /sevod-scraper/input # where to find the dump file for the dataset outputDumpDir : /sevod-scraper/output # where to place the configuration for the dataset # federator configuration step 2 (combination step): confImage : semagrow/semagrow-init-all # second docker image inputDir : /kobe/input # where to find all dataset configurations outputDir : /kobe/output # where to place the final (combined) configuration The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8080/SemaGrow/sparql , hence the port and the path to listen for queries are 8080 and /SemaGrow/sparql respectively. The input and output directories of the images mentioned previously are configured using the parameters inputDumpDir , outputDumpDir , inputDir , outputDir . Examples We have already prepared several federator template specifications to experiment with: federator-fedx federator-semagrow federator-uno Note We plan to define more federator template specifications in the future. We place all federator template specifications in the examples/ directory under a subdirectory with the prefix federator-* .","title":"Add data federators"},{"location":"extend/add_federator/#add-a-new-federator","text":"This walkthrough illustrates the steps required from the implementor of a federation engine in order to create a FederatorTemplate specification. In KOBE, a federator template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Federator templates are used in the Experiment specifications in order to define the federation engine to be benchmarked.","title":"Add a new federator"},{"location":"extend/add_federator/#prerequisites","text":"In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the federation engine (e.g., https://hub.docker.com/r/semagrow/semagrow/). Moreover, you should have a piece of software that automatically constructs the configuration required for your federator to operate (e.g., https://github.com/semagrow/sevod-scraper).","title":"Prerequisites"},{"location":"extend/add_federator/#step-1-prepare-your-docker-images","text":"Usually, a federation engine requires some configuration files that depend on the federated endpoints (e.g., the URLs of the federated SPARQL endpoints). Thus, apart from the Docker image with the SPARQL endpoint of the federation engine, you should provide a docker image that constructs any desired configuration for each of the source endpoints, and a docker image that initializes the federator that possibly takes into account the configuration files of the source endpoints. More specifically, prepare the following images: A docker image that constructs a configuration file for a source endpoint and places it in an output directory of your choice. Assume that the source endpoint and the dataset name are available in the environment variables $DATASET_NAME and $DATASET_URL respectively, and that the dump file of the dataset is present in an input directory of your choice. A docker image that constructs a configuration file for the federation engine and places it in an output directory of your choice. Assume that all the configuration files produced in the previous step are present in an input directory of your choice. A docker image that starts the federation engine and exposes its SPARQL endpoint. The environment variables are initialized by the KOBE operator according to the specification of the benchmark to be executed. As an example, we present the images for two federation engines (namely Fedx and Semagrow). Regarding the Semagrow federation engine, we use the images semagrow/semagrow-init (source code here ), semagrow/semagrow-init-all , (source code here ), and semagrow/semagrow (see here ). The first image uses the sevod-scraper tool to create a ttl metadata file from the dump file, and the second image concatenates all metadata files of each of the source endpoints into a single metadata file. Regarding the fedx federation engine, we use the images semagrow/fedx-init (source code here ), semagrow/fedx-init-all , (source code here ), and semagrow/fedx-server (source code here ). Fedx is known for not using any dataset statistics, but it uses only a ttl file that contains only the SPARQL endpoints of the federation. The first image creates a ttl file that defines the SPARQL endpoint of each dataset and the second image concatenates all ttl files of each source endpoints into a single configuration file.","title":"Step 1. Prepare your Docker images"},{"location":"extend/add_federator/#step-2-prepare-your-yaml-file","text":"Once you have prepared the docker images, creating the federator template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Semagrow): apiVersion : kobe.semagrow.org/v1alpha1 kind : FederatorTemplate metadata : # Each federator template can be uniquely identified by its name. name : semagrowtemplate spec : containers : # here you put the last image (that is the image for the # SPARQL endpoint of the federation engine) - name : maincontainer image : semagrow/semagrow ports : - containerPort : 8080 # port to listen for queries port : 8080 # port to listen for queries path : /SemaGrow/sparql # path to listen for queries fedConfDir : /etc/default/semagrow # where the federator expects to find its configuration # federator configuration step 1 (for each dataset): confFromFileImage : semagrow/semagrow-init # first docker image inputDumpDir : /sevod-scraper/input # where to find the dump file for the dataset outputDumpDir : /sevod-scraper/output # where to place the configuration for the dataset # federator configuration step 2 (combination step): confImage : semagrow/semagrow-init-all # second docker image inputDir : /kobe/input # where to find all dataset configurations outputDir : /kobe/output # where to place the final (combined) configuration The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8080/SemaGrow/sparql , hence the port and the path to listen for queries are 8080 and /SemaGrow/sparql respectively. The input and output directories of the images mentioned previously are configured using the parameters inputDumpDir , outputDumpDir , inputDir , outputDir .","title":"Step 2. Prepare your YAML file"},{"location":"extend/add_federator/#examples","text":"We have already prepared several federator template specifications to experiment with: federator-fedx federator-semagrow federator-uno Note We plan to define more federator template specifications in the future. We place all federator template specifications in the examples/ directory under a subdirectory with the prefix federator-* .","title":"Examples"},{"location":"extend/add_metrics/","text":"Add new metrics In this section we describe the process of adding new metrics in KOBE. Step 1. Produce a log message that contains the metric To add a new metric you must first modify your component to produce it. KOBE collects metrics by parsing log messages so to add the metric just add another log message in the component for this metric. Except from the metric, your log message should contain the following information: Experiment name Start time of the experiment Query name Run If the log message is produced by an evaluator, these parameters are obtained from the KOBE operator (see here ). If the log message is produced by a federator, these parameters are obtained by a SPARQL comment (see here ). Step 2. Configure Fluentd with a corresponding regex pattern All metrics are captured by Fluentd, so to collect the new metric you must edit the operator/deploy/efk-config/fluentd-values.yaml file and add another Fluentd filter at containers.input.conf for the introduced log pattern of the metric. For example if the log pattern is ^Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - MyMetric: (?<my_metric>[0-9]+)$ you must add filter <filter kubernetes.**> @type parser key_name message reserve_time true reserve_data true #suppress_parse_error_log true <parse> @type regexp expression ^Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - MyMetric: (?<my_metric>[0-9]+)$ types my_metric:integer </parse> </filter> After redeploying Fluentd, every time the metric log message is produced, it will be captured by Fluentd and stored in ElasticSearch. Step 3. Configure Kibana to visualize your metric Finally, you must add a new Kibana Dashboard to visualize the new metric, or add a Visualization on one of the existing Dashboards . Example As an example, check out the Fluentd configuration for the metrics we currently support.","title":"Add evaluation metrics"},{"location":"extend/add_metrics/#add-new-metrics","text":"In this section we describe the process of adding new metrics in KOBE.","title":"Add new metrics"},{"location":"extend/add_metrics/#step-1-produce-a-log-message-that-contains-the-metric","text":"To add a new metric you must first modify your component to produce it. KOBE collects metrics by parsing log messages so to add the metric just add another log message in the component for this metric. Except from the metric, your log message should contain the following information: Experiment name Start time of the experiment Query name Run If the log message is produced by an evaluator, these parameters are obtained from the KOBE operator (see here ). If the log message is produced by a federator, these parameters are obtained by a SPARQL comment (see here ).","title":"Step 1. Produce a log message that contains the metric"},{"location":"extend/add_metrics/#step-2-configure-fluentd-with-a-corresponding-regex-pattern","text":"All metrics are captured by Fluentd, so to collect the new metric you must edit the operator/deploy/efk-config/fluentd-values.yaml file and add another Fluentd filter at containers.input.conf for the introduced log pattern of the metric. For example if the log pattern is ^Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - MyMetric: (?<my_metric>[0-9]+)$ you must add filter <filter kubernetes.**> @type parser key_name message reserve_time true reserve_data true #suppress_parse_error_log true <parse> @type regexp expression ^Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - MyMetric: (?<my_metric>[0-9]+)$ types my_metric:integer </parse> </filter> After redeploying Fluentd, every time the metric log message is produced, it will be captured by Fluentd and stored in ElasticSearch.","title":"Step 2. Configure Fluentd with a corresponding regex pattern"},{"location":"extend/add_metrics/#step-3-configure-kibana-to-visualize-your-metric","text":"Finally, you must add a new Kibana Dashboard to visualize the new metric, or add a Visualization on one of the existing Dashboards .","title":"Step 3. Configure Kibana to visualize your metric"},{"location":"extend/add_metrics/#example","text":"As an example, check out the Fluentd configuration for the metrics we currently support.","title":"Example"},{"location":"extend/support_metrics/","text":"Provide metric support This walkthrough illustrates the steps required from the implementor of a federation engine in order to provide full support for the available benchmark metrics. Info This step is optional, in a sense that it is only needed in order to support all evaluation metrics of KOBE. KOBE will be able to visualize 1) the number of returned results and 2) the total time to receive the full result set to experimenter \"for free\" if you choose to not follow this step. Logging subsystem concepts and available metrics One important feature of KOBE is that the experimenter can have easy access on a set of several statistics and key performance indicators for each conducted experiment. The metrics currently supported are the following: Number of returned results Total time to receive the full result set Source selection time Query planning time Query execution time Number of sources accessed Of these evaluation metrics, only the first two can can be computed by the client side. Thus, the remaining metrics should be calculated by the federation engine itself and can be presented via a log message. However, in order for KOBE to be able to link the log message with its corresponding experiment execution and with its specific query run, the log message should contain also the following information: Experiment name Start time of the experiment Query name Run These parameters are passed from the evaluator of KOBE to the federation engine via a SPARQL comment that is attached in the query string. Step 1. Provide support for all evaluation metrics In order to provide full support for evaluation metrics, you should do the following: Extend your query string parser to parse the first line of the query string which follows the according regex pattern: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ Calculate some or all of the metrics discussed previously. Provide a log message to output the metrics according to the following regex pattern: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Source Selection Time: (?<source_selection_time>[0-9]+) - Compile Time: (?<compile_time>[0-9]+) - Sources: (?<sources>[0-9]+) - Execution time: (?<execution_time>[0-9]+)$ where <experiment> , <date> , <query> , <run> are obtained by the SPARQL comment of the query string. <source_selection_time> is the time to perform source selection. <compile_time> is the time to provide a query execution plan. <execution_time> is the time to execute the plan. <sources> is the number of sources that appear in the query plan. Note This guide describes the steps for the standard federator metrics in KOBE. If you want to add more metrics from the side of the federator, please refer to this guide . Example As an example, consider this pull request which contains the integration needed for KOBE in the case for the Semagrow federation engine.","title":"Provide metric support"},{"location":"extend/support_metrics/#provide-metric-support","text":"This walkthrough illustrates the steps required from the implementor of a federation engine in order to provide full support for the available benchmark metrics. Info This step is optional, in a sense that it is only needed in order to support all evaluation metrics of KOBE. KOBE will be able to visualize 1) the number of returned results and 2) the total time to receive the full result set to experimenter \"for free\" if you choose to not follow this step.","title":"Provide metric support"},{"location":"extend/support_metrics/#logging-subsystem-concepts-and-available-metrics","text":"One important feature of KOBE is that the experimenter can have easy access on a set of several statistics and key performance indicators for each conducted experiment. The metrics currently supported are the following: Number of returned results Total time to receive the full result set Source selection time Query planning time Query execution time Number of sources accessed Of these evaluation metrics, only the first two can can be computed by the client side. Thus, the remaining metrics should be calculated by the federation engine itself and can be presented via a log message. However, in order for KOBE to be able to link the log message with its corresponding experiment execution and with its specific query run, the log message should contain also the following information: Experiment name Start time of the experiment Query name Run These parameters are passed from the evaluator of KOBE to the federation engine via a SPARQL comment that is attached in the query string.","title":"Logging subsystem concepts and available metrics"},{"location":"extend/support_metrics/#step-1-provide-support-for-all-evaluation-metrics","text":"In order to provide full support for evaluation metrics, you should do the following: Extend your query string parser to parse the first line of the query string which follows the according regex pattern: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ Calculate some or all of the metrics discussed previously. Provide a log message to output the metrics according to the following regex pattern: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Source Selection Time: (?<source_selection_time>[0-9]+) - Compile Time: (?<compile_time>[0-9]+) - Sources: (?<sources>[0-9]+) - Execution time: (?<execution_time>[0-9]+)$ where <experiment> , <date> , <query> , <run> are obtained by the SPARQL comment of the query string. <source_selection_time> is the time to perform source selection. <compile_time> is the time to provide a query execution plan. <execution_time> is the time to execute the plan. <sources> is the number of sources that appear in the query plan. Note This guide describes the steps for the standard federator metrics in KOBE. If you want to add more metrics from the side of the federator, please refer to this guide .","title":"Step 1. Provide support for all evaluation metrics"},{"location":"extend/support_metrics/#example","text":"As an example, consider this pull request which contains the integration needed for KOBE in the case for the Semagrow federation engine.","title":"Example"},{"location":"getting_started/install/","text":"Installation This guide illustrates the steps required to install KOBE in your system. Prerequisites Kubernetes >= 1.8.0 nfs-commons installed in the nodes of the cluster. If in Debian or Ubuntu you can install it using apt-get install nfs-common Download KOBE To download KOBE in your system, just do the following: git clone https://github.com/semagrow/kobe.git cd kobe Installation of the Deployment subsystem KOBE needs the Kubernetes operator to be installed in the Kubernetes cluster. To quickly install the KOBE operator in a Kubernetes cluster. You can use the kobectl found in the bin directory: export PATH = ` pwd ` /bin: $PATH kobectl install operator . Alternatively, you could run the following commands: kubectl apply -f operator/deploy/crds kubectl apply -f operator/deploy/service_account.yaml kubectl apply -f operator/deploy/clusterrole.yaml kubectl apply -f operator/deploy/clusterrole_binding.yaml kubectl apply -f operator/deploy/role.yaml kubectl apply -f operator/deploy/operator.yaml You will get a confirmation message that each resource has successfully been created. This will set the operator running in your Kubernetes cluster and needs to be done only once. Installation of the Networking subsystem KOBE uses istio to support network delays between the different deployments. To install istio you can run the following: kobectl install istio . Alternatively, you can consult the official installation guide or you can type the following commands. curl -L https://istio.io/downloadIstio | sh - export PATH = ` pwd ` /istio-1.6.0/bin: $PATH istioctl manifest apply --set profile = default Installation of the Logging subsystem To enable the evaluation metrics extraction subsystem, run kobectl install efk . or alternatively the following helm repo add elastic https://helm.elastic.co helm repo add kiwigrid https://kiwigrid.github.io helm install elastic/elasticsearch --name elasticsearch --set persistence.enabled = false --set replicas = 1 --version 7 .6.2 helm install elastic/kibana --name kibana --set service.type = NodePort --version 7 .6.2 helm install --name fluentd -f operator/deploy/efk-config/fluentd-values.yaml kiwigrid/fluentd-elasticsearch --version 8 .0.1 kubectl apply -f operator/deploy/efk-config/kobe-kibana-configuration.yaml These result in the simplest setup of an one-node Elasticsearch that does not persist data across pod recreation, a Fluentd DaemonSet and a Kibana node that exposes a NodePort . The setup can be customized by changing the configuration parameters of each helm chart. Please check the corresponding documentation of each chart for more info.","title":"Installation"},{"location":"getting_started/install/#installation","text":"This guide illustrates the steps required to install KOBE in your system.","title":"Installation"},{"location":"getting_started/install/#prerequisites","text":"Kubernetes >= 1.8.0 nfs-commons installed in the nodes of the cluster. If in Debian or Ubuntu you can install it using apt-get install nfs-common","title":"Prerequisites"},{"location":"getting_started/install/#download-kobe","text":"To download KOBE in your system, just do the following: git clone https://github.com/semagrow/kobe.git cd kobe","title":"Download KOBE"},{"location":"getting_started/install/#installation-of-the-deployment-subsystem","text":"KOBE needs the Kubernetes operator to be installed in the Kubernetes cluster. To quickly install the KOBE operator in a Kubernetes cluster. You can use the kobectl found in the bin directory: export PATH = ` pwd ` /bin: $PATH kobectl install operator . Alternatively, you could run the following commands: kubectl apply -f operator/deploy/crds kubectl apply -f operator/deploy/service_account.yaml kubectl apply -f operator/deploy/clusterrole.yaml kubectl apply -f operator/deploy/clusterrole_binding.yaml kubectl apply -f operator/deploy/role.yaml kubectl apply -f operator/deploy/operator.yaml You will get a confirmation message that each resource has successfully been created. This will set the operator running in your Kubernetes cluster and needs to be done only once.","title":"Installation of the Deployment subsystem"},{"location":"getting_started/install/#installation-of-the-networking-subsystem","text":"KOBE uses istio to support network delays between the different deployments. To install istio you can run the following: kobectl install istio . Alternatively, you can consult the official installation guide or you can type the following commands. curl -L https://istio.io/downloadIstio | sh - export PATH = ` pwd ` /istio-1.6.0/bin: $PATH istioctl manifest apply --set profile = default","title":"Installation of the Networking subsystem"},{"location":"getting_started/install/#installation-of-the-logging-subsystem","text":"To enable the evaluation metrics extraction subsystem, run kobectl install efk . or alternatively the following helm repo add elastic https://helm.elastic.co helm repo add kiwigrid https://kiwigrid.github.io helm install elastic/elasticsearch --name elasticsearch --set persistence.enabled = false --set replicas = 1 --version 7 .6.2 helm install elastic/kibana --name kibana --set service.type = NodePort --version 7 .6.2 helm install --name fluentd -f operator/deploy/efk-config/fluentd-values.yaml kiwigrid/fluentd-elasticsearch --version 8 .0.1 kubectl apply -f operator/deploy/efk-config/kobe-kibana-configuration.yaml These result in the simplest setup of an one-node Elasticsearch that does not persist data across pod recreation, a Fluentd DaemonSet and a Kibana node that exposes a NodePort . The setup can be customized by changing the configuration parameters of each helm chart. Please check the corresponding documentation of each chart for more info.","title":"Installation of the Logging subsystem"},{"location":"getting_started/run_experiment/","text":"Perform a benchmark experiment In the following, we show the steps for deploying an experiment on a simple benchmark that comprises three queries over a semagrow federation of two Virtuoso endpoints. You can use the kobectl found in the bin directory for controlling your experiments: export PATH=`pwd`/bin:$PATH kobectl help First, apply the templates for Virtuoso and semagrow : kobectl apply examples/dataset-virtuoso/virtuosotemplate.yaml kobectl apply examples/federator-semagrow/semagrowtemplate.yaml Then, apply the benchmark. kobectl apply examples/benchmark-toybench/toybench-simple.yaml Before running the experiment, you should verify that the datasets are loaded. Use the following command: kobectl show benchmark toybench-simple When the datasets are loaded, you should get the following output: NAME STATUS toy1 Running toy2 Running Proceed now with the execution of the experiment: kobectl apply examples/experiment-toybench/toyexp-simple.yaml As previously, you can review the state of the experiment with the following command: kobectl show experiment toyexp-simple You can now view the evaluation metrics in the Kibana dashboards. For removing all of the above, issue the following commands: kobectl delete experiment toyexp-simple kobectl delete benchmark toybench-simple kobectl delete federatortemplate semagrowtemplate kobectl delete datasettemplate virtuosotemplate For more advanced control options for KOBE, use kubectl .","title":"Perform an experiment"},{"location":"getting_started/run_experiment/#perform-a-benchmark-experiment","text":"In the following, we show the steps for deploying an experiment on a simple benchmark that comprises three queries over a semagrow federation of two Virtuoso endpoints. You can use the kobectl found in the bin directory for controlling your experiments: export PATH=`pwd`/bin:$PATH kobectl help First, apply the templates for Virtuoso and semagrow : kobectl apply examples/dataset-virtuoso/virtuosotemplate.yaml kobectl apply examples/federator-semagrow/semagrowtemplate.yaml Then, apply the benchmark. kobectl apply examples/benchmark-toybench/toybench-simple.yaml Before running the experiment, you should verify that the datasets are loaded. Use the following command: kobectl show benchmark toybench-simple When the datasets are loaded, you should get the following output: NAME STATUS toy1 Running toy2 Running Proceed now with the execution of the experiment: kobectl apply examples/experiment-toybench/toyexp-simple.yaml As previously, you can review the state of the experiment with the following command: kobectl show experiment toyexp-simple You can now view the evaluation metrics in the Kibana dashboards. For removing all of the above, issue the following commands: kobectl delete experiment toyexp-simple kobectl delete benchmark toybench-simple kobectl delete federatortemplate semagrowtemplate kobectl delete datasettemplate virtuosotemplate For more advanced control options for KOBE, use kubectl .","title":"Perform a benchmark experiment"},{"location":"getting_started/view_results/","text":"Evaluate the results This guide illustrates how to view the results of the benchmark. Viewing the Kibana dashboards In order to view the dashboards you should have installed the Logging subsystem of KOBE. After all pods are in Running state Kibana dashboards can be accessed at http://<NODE-IP>:<NODEPORT>/app/kibana#/dashboard/ where <NODE-IP> the IP of any of the Kubernetes cluster nodes and <NODEPORT> the result of kubectl get -o jsonpath = \"{.spec.ports[0].nodePort}\" services kibana-kibana There are three available dashboards, which can be shown below. Details of a specific experiment execution The first dashboard focuses on a specific experiment execution. It comprises: Time of each phase of the query processing for each query of the experiment. Total time to receive the complete result set for each query of the experiment. Number of sources accessed for each query of the experiment. Number of returned results for each query of the experiment. The first and the third visualizations are obtained from the logs of the federator engine, if available. The second and the fourth visualizations are obtained from the logs of the evaluator, so they are available even for federators that do not provide KOBE-specific logs. The values in each visualization can be also exported in a CSV file for further processing. Comparisons of experiment runs The remaining two dashboards can be used to draw comparisons between several experiment runs in order to directly compare different configurations of a benchmark. The following dashboard can be used for comparing several experiment executions. It consists of two visualizations: Total time to receive the complete result set for each experiment execution. Number of returned results for each specified experiment execution. Each bar refers to a single query of the experiments presented. Finally, the last dashboard displays plays the same metrics. The main difference though, is that it focuses on a specific query and compares all runs of this query for several experiment executions. Contrary to the visualizations of the other two dashboards, each bar refers to a single experiment run, and all runs are grouped according to the experiment execution they belong. These visualizations are obtained from the logs of the evaluator.","title":"Evaluate the results"},{"location":"getting_started/view_results/#evaluate-the-results","text":"This guide illustrates how to view the results of the benchmark.","title":"Evaluate the results"},{"location":"getting_started/view_results/#viewing-the-kibana-dashboards","text":"In order to view the dashboards you should have installed the Logging subsystem of KOBE. After all pods are in Running state Kibana dashboards can be accessed at http://<NODE-IP>:<NODEPORT>/app/kibana#/dashboard/ where <NODE-IP> the IP of any of the Kubernetes cluster nodes and <NODEPORT> the result of kubectl get -o jsonpath = \"{.spec.ports[0].nodePort}\" services kibana-kibana There are three available dashboards, which can be shown below.","title":"Viewing the Kibana dashboards"},{"location":"getting_started/view_results/#details-of-a-specific-experiment-execution","text":"The first dashboard focuses on a specific experiment execution. It comprises: Time of each phase of the query processing for each query of the experiment. Total time to receive the complete result set for each query of the experiment. Number of sources accessed for each query of the experiment. Number of returned results for each query of the experiment. The first and the third visualizations are obtained from the logs of the federator engine, if available. The second and the fourth visualizations are obtained from the logs of the evaluator, so they are available even for federators that do not provide KOBE-specific logs. The values in each visualization can be also exported in a CSV file for further processing.","title":"Details of a specific experiment execution"},{"location":"getting_started/view_results/#comparisons-of-experiment-runs","text":"The remaining two dashboards can be used to draw comparisons between several experiment runs in order to directly compare different configurations of a benchmark. The following dashboard can be used for comparing several experiment executions. It consists of two visualizations: Total time to receive the complete result set for each experiment execution. Number of returned results for each specified experiment execution. Each bar refers to a single query of the experiments presented. Finally, the last dashboard displays plays the same metrics. The main difference though, is that it focuses on a specific query and compares all runs of this query for several experiment executions. Contrary to the visualizations of the other two dashboards, each bar refers to a single experiment run, and all runs are grouped according to the experiment execution they belong. These visualizations are obtained from the logs of the evaluator.","title":"Comparisons of experiment runs"},{"location":"getting_started/workflow/","text":"Typical workflow The typical workflow of defining a KOBE experiment is the following. Create one DatasetTemplate for each dataset server you want to use in your benchmark. Define your Benchmark , which should contain a list of datasets and a list of queries. Create one FederatorTemplate for the federator engine you want to use in your experiment. Define an Experiment over your previously defined benchmark. Several examples of the above specifications can be found in the examples directory.","title":"Workflow"},{"location":"getting_started/workflow/#typical-workflow","text":"The typical workflow of defining a KOBE experiment is the following. Create one DatasetTemplate for each dataset server you want to use in your benchmark. Define your Benchmark , which should contain a list of datasets and a list of queries. Create one FederatorTemplate for the federator engine you want to use in your experiment. Define an Experiment over your previously defined benchmark. Several examples of the above specifications can be found in the examples directory.","title":"Typical workflow"},{"location":"references/api/","text":"API Docs This Document documents the types introduced by the Kobe Operator to be consumed by users. Note this document is generated from code comments. When contributing a change to this document please do so by changing the code comments. Table of Contents Benchmark BenchmarkList BenchmarkSpec Dataset DatasetEndpoint DatasetFile DatasetTemplate DatasetTemplateList Delay EphemeralDataset EphemeralDatasetList EphemeralDatasetStatus Evaluator Experiment ExperimentList ExperimentSpec ExperimentStatus Federation FederationList FederationSpec FederationStatus Federator FederatorList FederatorSpec FederatorTemplate FederatorTemplateList KobeUtil KobeUtilList NetworkConnection Query SystemDatasetSpec Benchmark Benchmark is the Schema for the benchmarks API Field Description Scheme Required metadata metav1.ObjectMeta false spec BenchmarkSpec false status BenchmarkStatus false Back to TOC BenchmarkList BenchmarkList contains a list of Benchmark Field Description Scheme Required metadata metav1.ListMeta false items [] Benchmark true Back to TOC BenchmarkSpec BenchmarkSpec defines the components for this benchmark setup Field Description Scheme Required datasets [] Dataset true queries [] Query true Back to TOC Dataset Field Description Scheme Required name string true files [] DatasetFile true systemspec * SystemDatasetSpec false templateRef string false affinity If specified, the pod's scheduling constraints * v1.Affinity false resources Resources are not allowed for ephemeral containers. Ephemeral containers use spare resources already allocated to the pod. v1.ResourceRequirements false networkTopology network delays [] NetworkConnection false federatorConnection * NetworkConnection false Back to TOC DatasetEndpoint Field Description Scheme Required host string true namespace string true port uint32 true path string true Back to TOC DatasetFile Field Description Scheme Required url string true checksum string false Back to TOC DatasetTemplate Field Description Scheme Required metadata metav1.ObjectMeta false spec SystemDatasetSpec false Back to TOC DatasetTemplateList FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] DatasetTemplate true Back to TOC Delay Field Description Scheme Required fixedDelaySec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false fixedDelayMSec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false percentage *uint32 false percent *uint32 false Back to TOC EphemeralDataset EphemeralDataset is the Schema for the kobedatasets API Field Description Scheme Required metadata metav1.ObjectMeta false spec Dataset false status EphemeralDatasetStatus false Back to TOC EphemeralDatasetList EphemeralDatasetList contains a list of EphemeralDataset Field Description Scheme Required metadata metav1.ListMeta false items [] EphemeralDataset true Back to TOC EphemeralDatasetStatus Field Description Scheme Required podNames []string true phase string true forceLoad bool true Back to TOC Evaluator Evaluator defines the Field Description Scheme Required image string true imagePullPolicy v1.PullPolicy false command []string false parallelism int32 false env [] v1.EnvVar false Back to TOC Experiment Experiment is the Schema for the experiments API Field Description Scheme Required metadata metav1.ObjectMeta false spec ExperimentSpec false status ExperimentStatus false Back to TOC ExperimentList ExperimentList contains a list of Experiment Field Description Scheme Required metadata metav1.ListMeta false items [] Experiment true Back to TOC ExperimentSpec ExperimentSpec defines the desired state of Experiment Field Description Scheme Required benchmark string true federatorName string true federatorSpec * FederatorSpec false federatorTemplateRef string false evaluator Evaluator true timesToRun int true restartPolicy RestartPolicy false dryRun bool true forceNewInit bool true Back to TOC ExperimentStatus ExperimentStatus defines the observed state of Experiment Field Description Scheme Required startTime Time at which this workflow started metav1.Time false completionTime Time at which this workflow completed metav1.Time false run The current iteration of the experiment It should be zero if not started yet int false phase The phase of the experiment ExperimentPhase true Back to TOC Federation Federation is the Schema for the federations API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederationSpec false status FederationStatus false Back to TOC FederationList FederationList contains a list of Federation Field Description Scheme Required metadata metav1.ListMeta false items [] Federation true Back to TOC FederationSpec FederationSpec defines the desired state of Federation Field Description Scheme Required federatorName string true spec FederatorSpec true datasets [] DatasetEndpoint true topology [] NetworkConnection false initPolicy InitializationPolicy false Back to TOC FederationStatus FederationStatus defines the observed state of KobeFederation Field Description Scheme Required podNames []string true phase FederationPhase true Back to TOC Federator Federator is the Schema for the federators API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC FederatorList FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] Federator true Back to TOC FederatorSpec FederatorSpec contains all necessary information for a federator Field Description Scheme Required initContainers []v1.Container false containers []v1.Container true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. int32 true path suffix to be added to endpoint of federator f.e ../SemaGrow/sparql string true confFromFileImage The Docker image that receives a compressed dataset and may produce configuration needed from the federator to federate this specific dataset This container will run one time for each dataset in the federation string true inputDumpDir where the above image expects the dump to be (if from dump) string true outputDumpDir where the above image will place its result config file string true confImage The Docker image that initializes the federator (equivalent to initContainers) string true inputDir string true outputDir string true fedConfDir which directory the federator needs the metadata config files in order to find them string true Back to TOC FederatorTemplate FederatorTemplate defines a federator and its components that it needs to be installed. Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC FederatorTemplateList Field Description Scheme Required metadata metav1.ListMeta false items [] FederatorTemplate true Back to TOC KobeUtil KobeUtil is the Schema for the kobeutils API Field Description Scheme Required metadata metav1.ObjectMeta false Back to TOC KobeUtilList KobeUtilList contains a list of KobeUtil Field Description Scheme Required metadata metav1.ListMeta false items [] KobeUtil true Back to TOC NetworkConnection Field Description Scheme Required datasetSource *string false delayInjection Delay false Back to TOC Query Query contains the query info Field Description Scheme Required name string true language string true queryString string true Back to TOC SystemDatasetSpec DatasetSpec defines the desired state of Dataset Field Description Scheme Required importContainers []v1.Container false initContainers List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started. If any init container fails, the pod is considered to have failed and is handled according to its restartPolicy. The name for an init container or normal container must be unique among all containers. Init containers may not have Lifecycle actions, Readiness probes, or Liveness probes. The resourceRequirements of an init container are taken into account during scheduling by finding the highest request/limit for each resource type, and then using the max of of that value or the sum of the normal containers. Limits are applied to init containers in a similar fashion. Init containers cannot currently be added or removed. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ []v1.Container false containers List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. []v1.Container true initPolicy Forces to download and load from dataset file InitializationPolicy true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. uint32 true path Path that the container will listen for queries string true Back to TOC","title":"API"},{"location":"references/api/#api-docs","text":"This Document documents the types introduced by the Kobe Operator to be consumed by users. Note this document is generated from code comments. When contributing a change to this document please do so by changing the code comments.","title":"API Docs"},{"location":"references/api/#table-of-contents","text":"Benchmark BenchmarkList BenchmarkSpec Dataset DatasetEndpoint DatasetFile DatasetTemplate DatasetTemplateList Delay EphemeralDataset EphemeralDatasetList EphemeralDatasetStatus Evaluator Experiment ExperimentList ExperimentSpec ExperimentStatus Federation FederationList FederationSpec FederationStatus Federator FederatorList FederatorSpec FederatorTemplate FederatorTemplateList KobeUtil KobeUtilList NetworkConnection Query SystemDatasetSpec","title":"Table of Contents"},{"location":"references/api/#benchmark","text":"Benchmark is the Schema for the benchmarks API Field Description Scheme Required metadata metav1.ObjectMeta false spec BenchmarkSpec false status BenchmarkStatus false Back to TOC","title":"Benchmark"},{"location":"references/api/#benchmarklist","text":"BenchmarkList contains a list of Benchmark Field Description Scheme Required metadata metav1.ListMeta false items [] Benchmark true Back to TOC","title":"BenchmarkList"},{"location":"references/api/#benchmarkspec","text":"BenchmarkSpec defines the components for this benchmark setup Field Description Scheme Required datasets [] Dataset true queries [] Query true Back to TOC","title":"BenchmarkSpec"},{"location":"references/api/#dataset","text":"Field Description Scheme Required name string true files [] DatasetFile true systemspec * SystemDatasetSpec false templateRef string false affinity If specified, the pod's scheduling constraints * v1.Affinity false resources Resources are not allowed for ephemeral containers. Ephemeral containers use spare resources already allocated to the pod. v1.ResourceRequirements false networkTopology network delays [] NetworkConnection false federatorConnection * NetworkConnection false Back to TOC","title":"Dataset"},{"location":"references/api/#datasetendpoint","text":"Field Description Scheme Required host string true namespace string true port uint32 true path string true Back to TOC","title":"DatasetEndpoint"},{"location":"references/api/#datasetfile","text":"Field Description Scheme Required url string true checksum string false Back to TOC","title":"DatasetFile"},{"location":"references/api/#datasettemplate","text":"Field Description Scheme Required metadata metav1.ObjectMeta false spec SystemDatasetSpec false Back to TOC","title":"DatasetTemplate"},{"location":"references/api/#datasettemplatelist","text":"FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] DatasetTemplate true Back to TOC","title":"DatasetTemplateList"},{"location":"references/api/#delay","text":"Field Description Scheme Required fixedDelaySec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false fixedDelayMSec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false percentage *uint32 false percent *uint32 false Back to TOC","title":"Delay"},{"location":"references/api/#ephemeraldataset","text":"EphemeralDataset is the Schema for the kobedatasets API Field Description Scheme Required metadata metav1.ObjectMeta false spec Dataset false status EphemeralDatasetStatus false Back to TOC","title":"EphemeralDataset"},{"location":"references/api/#ephemeraldatasetlist","text":"EphemeralDatasetList contains a list of EphemeralDataset Field Description Scheme Required metadata metav1.ListMeta false items [] EphemeralDataset true Back to TOC","title":"EphemeralDatasetList"},{"location":"references/api/#ephemeraldatasetstatus","text":"Field Description Scheme Required podNames []string true phase string true forceLoad bool true Back to TOC","title":"EphemeralDatasetStatus"},{"location":"references/api/#evaluator","text":"Evaluator defines the Field Description Scheme Required image string true imagePullPolicy v1.PullPolicy false command []string false parallelism int32 false env [] v1.EnvVar false Back to TOC","title":"Evaluator"},{"location":"references/api/#experiment","text":"Experiment is the Schema for the experiments API Field Description Scheme Required metadata metav1.ObjectMeta false spec ExperimentSpec false status ExperimentStatus false Back to TOC","title":"Experiment"},{"location":"references/api/#experimentlist","text":"ExperimentList contains a list of Experiment Field Description Scheme Required metadata metav1.ListMeta false items [] Experiment true Back to TOC","title":"ExperimentList"},{"location":"references/api/#experimentspec","text":"ExperimentSpec defines the desired state of Experiment Field Description Scheme Required benchmark string true federatorName string true federatorSpec * FederatorSpec false federatorTemplateRef string false evaluator Evaluator true timesToRun int true restartPolicy RestartPolicy false dryRun bool true forceNewInit bool true Back to TOC","title":"ExperimentSpec"},{"location":"references/api/#experimentstatus","text":"ExperimentStatus defines the observed state of Experiment Field Description Scheme Required startTime Time at which this workflow started metav1.Time false completionTime Time at which this workflow completed metav1.Time false run The current iteration of the experiment It should be zero if not started yet int false phase The phase of the experiment ExperimentPhase true Back to TOC","title":"ExperimentStatus"},{"location":"references/api/#federation","text":"Federation is the Schema for the federations API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederationSpec false status FederationStatus false Back to TOC","title":"Federation"},{"location":"references/api/#federationlist","text":"FederationList contains a list of Federation Field Description Scheme Required metadata metav1.ListMeta false items [] Federation true Back to TOC","title":"FederationList"},{"location":"references/api/#federationspec","text":"FederationSpec defines the desired state of Federation Field Description Scheme Required federatorName string true spec FederatorSpec true datasets [] DatasetEndpoint true topology [] NetworkConnection false initPolicy InitializationPolicy false Back to TOC","title":"FederationSpec"},{"location":"references/api/#federationstatus","text":"FederationStatus defines the observed state of KobeFederation Field Description Scheme Required podNames []string true phase FederationPhase true Back to TOC","title":"FederationStatus"},{"location":"references/api/#federator","text":"Federator is the Schema for the federators API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC","title":"Federator"},{"location":"references/api/#federatorlist","text":"FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] Federator true Back to TOC","title":"FederatorList"},{"location":"references/api/#federatorspec","text":"FederatorSpec contains all necessary information for a federator Field Description Scheme Required initContainers []v1.Container false containers []v1.Container true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. int32 true path suffix to be added to endpoint of federator f.e ../SemaGrow/sparql string true confFromFileImage The Docker image that receives a compressed dataset and may produce configuration needed from the federator to federate this specific dataset This container will run one time for each dataset in the federation string true inputDumpDir where the above image expects the dump to be (if from dump) string true outputDumpDir where the above image will place its result config file string true confImage The Docker image that initializes the federator (equivalent to initContainers) string true inputDir string true outputDir string true fedConfDir which directory the federator needs the metadata config files in order to find them string true Back to TOC","title":"FederatorSpec"},{"location":"references/api/#federatortemplate","text":"FederatorTemplate defines a federator and its components that it needs to be installed. Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC","title":"FederatorTemplate"},{"location":"references/api/#federatortemplatelist","text":"Field Description Scheme Required metadata metav1.ListMeta false items [] FederatorTemplate true Back to TOC","title":"FederatorTemplateList"},{"location":"references/api/#kobeutil","text":"KobeUtil is the Schema for the kobeutils API Field Description Scheme Required metadata metav1.ObjectMeta false Back to TOC","title":"KobeUtil"},{"location":"references/api/#kobeutillist","text":"KobeUtilList contains a list of KobeUtil Field Description Scheme Required metadata metav1.ListMeta false items [] KobeUtil true Back to TOC","title":"KobeUtilList"},{"location":"references/api/#networkconnection","text":"Field Description Scheme Required datasetSource *string false delayInjection Delay false Back to TOC","title":"NetworkConnection"},{"location":"references/api/#query","text":"Query contains the query info Field Description Scheme Required name string true language string true queryString string true Back to TOC","title":"Query"},{"location":"references/api/#systemdatasetspec","text":"DatasetSpec defines the desired state of Dataset Field Description Scheme Required importContainers []v1.Container false initContainers List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started. If any init container fails, the pod is considered to have failed and is handled according to its restartPolicy. The name for an init container or normal container must be unique among all containers. Init containers may not have Lifecycle actions, Readiness probes, or Liveness probes. The resourceRequirements of an init container are taken into account during scheduling by finding the highest request/limit for each resource type, and then using the max of of that value or the sum of the normal containers. Limits are applied to init containers in a similar fashion. Init containers cannot currently be added or removed. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ []v1.Container false containers List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. []v1.Container true initPolicy Forces to download and load from dataset file InitializationPolicy true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. uint32 true path Path that the container will listen for queries string true Back to TOC","title":"SystemDatasetSpec"},{"location":"references/components/","text":"","title":"Components"},{"location":"references/kobectl/","text":"kobectl kobectl controls the KOBE open benchmarking engine. Installation kobectl is a sh script and can be found in the bin . Essentially, kobectl is a wrapper of Kubernetes commands so kubectl must be installed and available. You can make kobectl accessible to your path by export PATH = \" $( pwd ) /bin: $PATH \" Commands Command Explanation apply apply a resource using a .yaml configuration file get display all resources of specific type show show the state of a benchmark or an experiment delete delete a resource of specific type install install KOBE components purge uninstall KOBE help print a help message Usage kobectl apply [configuration_file] kobectl get [resource_type] kobectl show [resource_type] [resource] kobectl delete [resource_type] [resource] kobectl install [component] [kobe-directory] kobectl purge [kobe-directory] [resource_type] can be any of: benchmark(s) , experiment(s) , federatortemplate(s) , datasettemplate(s) . [component] can be any of: operator , operator-v1 , operator-v1beta1 , istio , efk , full Other For more advanced control options for KOBE, use kubectl .","title":"Command line"},{"location":"references/kobectl/#kobectl","text":"kobectl controls the KOBE open benchmarking engine.","title":"kobectl"},{"location":"references/kobectl/#installation","text":"kobectl is a sh script and can be found in the bin . Essentially, kobectl is a wrapper of Kubernetes commands so kubectl must be installed and available. You can make kobectl accessible to your path by export PATH = \" $( pwd ) /bin: $PATH \"","title":"Installation"},{"location":"references/kobectl/#commands","text":"Command Explanation apply apply a resource using a .yaml configuration file get display all resources of specific type show show the state of a benchmark or an experiment delete delete a resource of specific type install install KOBE components purge uninstall KOBE help print a help message","title":"Commands"},{"location":"references/kobectl/#usage","text":"kobectl apply [configuration_file] kobectl get [resource_type] kobectl show [resource_type] [resource] kobectl delete [resource_type] [resource] kobectl install [component] [kobe-directory] kobectl purge [kobe-directory] [resource_type] can be any of: benchmark(s) , experiment(s) , federatortemplate(s) , datasettemplate(s) . [component] can be any of: operator , operator-v1 , operator-v1beta1 , istio , efk , full","title":"Usage"},{"location":"references/kobectl/#other","text":"For more advanced control options for KOBE, use kubectl .","title":"Other"},{"location":"use/create_benchmark/","text":"Create a new benchmark This walkthrough illustrates the steps required from the benchmark designer in order to create a Benchmark specification. In KOBE, a benchmark comprises a collection of data sources, the latency of these data sources, and a list of query strings. Benchmarks are defined independently of the federator that is being benchmarked. Prerequisites In this walkthrough we assume that you already have already prepared the following: The dump of each RDF dataset of the benchmark. A list of query strings of the benchmark. A DatasetTemplate for each dataset server you want to use in your benchmark. Regarding the third prerequisite, we have already prepared several dataset templates to use. If you want to create your own dataset server template, check out this guide . Step 1. Prepare your dataset dumps Create a .tar.gz file for each dataset, and upload it on a known location. Place all files of the dataset into a directory, put this directory into a tar file and compress it with gzip. Even though most dataset engines support the import of several RDF formats (such as RDF/XML, turtle, etc), the most simple format is N-TRIPLES. Therefore, we suggest to store your dataset in a single .nt file. If you choose to to prepare a dump.nt file, just do the following: mkdir dataset/ mv dump.nt dataset/ tar czvf dataset.tar.gz dataset/ Finally, upload the .tar.gz file on a known location. As an example, we have uploaded the datasets for the FedBench experiment in the following location . Step 2. Prepare your YAML file A benchmark is characterized by its name and is parameterized using a list of datasets and a set of queries . A typical benchmark specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Benchmark metadata : # Each benchmark can be uniquely identified by its name. name : mybench spec : # Each benchmark consists of a set of dataset specifications. datasets : # Each dataset can be uniquely identified by its name, # and is defined with # A list of URLs that contain the dump of the dataset to download. # A specification of the dataset server to use (dataset template). - name : dataset1 files : - url : https://path/to/download/the/dataset1.tar.gz templateRef : datasettemplate # ... add more datasets ... # Each benchmark consists of a set of queries. queries : # Each query can be uniquely identified by its name, # and is defined with # The language in which the query is written (e.g., SPARQL). # The actual query string to be posed to the federator. - name : query1 language : sparql queryString : \"SELECT * WHERE ... \" # ... add more queries ... Check the following link in which we illustrate a simple example of the above specification: benchmark-toybench/toybench-simple.yaml This benchmark contains three SPARQL queries (namely tq1 , tq2 , and tq3 ), and two datasets (namely toy1 and toy2 ), both of them served by Virtuoso. Examples We have already prepared several benchmark specifications to experiment with: benchmark-fedbench benchmark-geofedbench benchmark-geographica benchmark-toybench Note We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Create benchmarks"},{"location":"use/create_benchmark/#create-a-new-benchmark","text":"This walkthrough illustrates the steps required from the benchmark designer in order to create a Benchmark specification. In KOBE, a benchmark comprises a collection of data sources, the latency of these data sources, and a list of query strings. Benchmarks are defined independently of the federator that is being benchmarked.","title":"Create a new benchmark"},{"location":"use/create_benchmark/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: The dump of each RDF dataset of the benchmark. A list of query strings of the benchmark. A DatasetTemplate for each dataset server you want to use in your benchmark. Regarding the third prerequisite, we have already prepared several dataset templates to use. If you want to create your own dataset server template, check out this guide .","title":"Prerequisites"},{"location":"use/create_benchmark/#step-1-prepare-your-dataset-dumps","text":"Create a .tar.gz file for each dataset, and upload it on a known location. Place all files of the dataset into a directory, put this directory into a tar file and compress it with gzip. Even though most dataset engines support the import of several RDF formats (such as RDF/XML, turtle, etc), the most simple format is N-TRIPLES. Therefore, we suggest to store your dataset in a single .nt file. If you choose to to prepare a dump.nt file, just do the following: mkdir dataset/ mv dump.nt dataset/ tar czvf dataset.tar.gz dataset/ Finally, upload the .tar.gz file on a known location. As an example, we have uploaded the datasets for the FedBench experiment in the following location .","title":"Step 1. Prepare your dataset dumps"},{"location":"use/create_benchmark/#step-2-prepare-your-yaml-file","text":"A benchmark is characterized by its name and is parameterized using a list of datasets and a set of queries . A typical benchmark specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Benchmark metadata : # Each benchmark can be uniquely identified by its name. name : mybench spec : # Each benchmark consists of a set of dataset specifications. datasets : # Each dataset can be uniquely identified by its name, # and is defined with # A list of URLs that contain the dump of the dataset to download. # A specification of the dataset server to use (dataset template). - name : dataset1 files : - url : https://path/to/download/the/dataset1.tar.gz templateRef : datasettemplate # ... add more datasets ... # Each benchmark consists of a set of queries. queries : # Each query can be uniquely identified by its name, # and is defined with # The language in which the query is written (e.g., SPARQL). # The actual query string to be posed to the federator. - name : query1 language : sparql queryString : \"SELECT * WHERE ... \" # ... add more queries ... Check the following link in which we illustrate a simple example of the above specification: benchmark-toybench/toybench-simple.yaml This benchmark contains three SPARQL queries (namely tq1 , tq2 , and tq3 ), and two datasets (namely toy1 and toy2 ), both of them served by Virtuoso.","title":"Step 2. Prepare your YAML file"},{"location":"use/create_benchmark/#examples","text":"We have already prepared several benchmark specifications to experiment with: benchmark-fedbench benchmark-geofedbench benchmark-geographica benchmark-toybench Note We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Examples"},{"location":"use/create_experiment/","text":"Create a new experiment This walkthrough illustrates the steps required from the experimenter in order to create an Experiment specification. In KOBE, an experiment is defined over a benchmark and federator. This resource provides the necessary parameters for instantiating a federation of querying systems. Prerequisites In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. A FederatorTemplate for the federation engine you want to use in your experiment. A docker image of the evaluator, which is a piece of software that will pose the queries to the federator. We have already prepared several benchmarks and federator templates to use. If you want to create your own benchmark, check out this guide . Moreover, if you want to create your own federator template, check out this guide . Regarding the evaluator, we currently we offer the docker image semagrow/kobe-sequential-evaluator , which executes the queries of the benchmark in a sequential manner. If you want to create your own evaluator, check out this guide . Step 1. Prepare your YAML file An experiment is characterized by its name and is parameterized with a benchmark to be executed and a federator template to be used. A typical experiment specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Experiment metadata : # Each experiment can be uniquely identified by its name. name : myexperiment spec : # Specify the name of the benchmark to be executed. benchmark : mybench # Specify the name of the federation engine of the experiment. federatorName : myfederator # Specify the name of the federator template to be used. federatorTemplateRef : federatortemplate # Specify the docker image of the evaluator. evaluator : image : semagrow/kobe-sequential-evaluator # Specify the number of runs of the experiment, i.e. how many times each query # of the benchmark should be executed. timesToRun : runs # If you set this parameter to true, KOBE will only build the federation # and will not start the experiment. dryRun : false # If you set this parameter to false, KOBE will not build the federation # if it was already built in previous executions of this experiment. forceNewInit : true Check the following link in which we illustrate a simple example of the above specification: experiment-toybench/toyexp-simple.yaml In this example, we define an experiment over the toybench-simple benchmark, and we use the Semagrow federation engine. The queries of the benchmark are executed in a sequential manner, and each query of the benchmark is executed 3 times. Since toybench-simple contains the queries tq1 , tq2 , tq3 , in the example experiment the queries will be executed with the following order: tq1 , tq2 , tq3 , tq1 , tq2 , tq3 , tq1 , tq2 , tq3 . Examples We have already prepared several experiment specifications to experiment with: experiment-fedbench experiment-geofedbench experiment-geographica experiment-toybench Note We plan to define more experiment specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix experiment-* .","title":"Create experiments"},{"location":"use/create_experiment/#create-a-new-experiment","text":"This walkthrough illustrates the steps required from the experimenter in order to create an Experiment specification. In KOBE, an experiment is defined over a benchmark and federator. This resource provides the necessary parameters for instantiating a federation of querying systems.","title":"Create a new experiment"},{"location":"use/create_experiment/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. A FederatorTemplate for the federation engine you want to use in your experiment. A docker image of the evaluator, which is a piece of software that will pose the queries to the federator. We have already prepared several benchmarks and federator templates to use. If you want to create your own benchmark, check out this guide . Moreover, if you want to create your own federator template, check out this guide . Regarding the evaluator, we currently we offer the docker image semagrow/kobe-sequential-evaluator , which executes the queries of the benchmark in a sequential manner. If you want to create your own evaluator, check out this guide .","title":"Prerequisites"},{"location":"use/create_experiment/#step-1-prepare-your-yaml-file","text":"An experiment is characterized by its name and is parameterized with a benchmark to be executed and a federator template to be used. A typical experiment specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Experiment metadata : # Each experiment can be uniquely identified by its name. name : myexperiment spec : # Specify the name of the benchmark to be executed. benchmark : mybench # Specify the name of the federation engine of the experiment. federatorName : myfederator # Specify the name of the federator template to be used. federatorTemplateRef : federatortemplate # Specify the docker image of the evaluator. evaluator : image : semagrow/kobe-sequential-evaluator # Specify the number of runs of the experiment, i.e. how many times each query # of the benchmark should be executed. timesToRun : runs # If you set this parameter to true, KOBE will only build the federation # and will not start the experiment. dryRun : false # If you set this parameter to false, KOBE will not build the federation # if it was already built in previous executions of this experiment. forceNewInit : true Check the following link in which we illustrate a simple example of the above specification: experiment-toybench/toyexp-simple.yaml In this example, we define an experiment over the toybench-simple benchmark, and we use the Semagrow federation engine. The queries of the benchmark are executed in a sequential manner, and each query of the benchmark is executed 3 times. Since toybench-simple contains the queries tq1 , tq2 , tq3 , in the example experiment the queries will be executed with the following order: tq1 , tq2 , tq3 , tq1 , tq2 , tq3 , tq1 , tq2 , tq3 .","title":"Step 1. Prepare your YAML file"},{"location":"use/create_experiment/#examples","text":"We have already prepared several experiment specifications to experiment with: experiment-fedbench experiment-geofedbench experiment-geographica experiment-toybench Note We plan to define more experiment specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix experiment-* .","title":"Examples"},{"location":"use/tune_network/","text":"Tune network settings This walkthrough illustrates the steps required from the benchmark designer in order to configure the latency of the data sources of a Benchmark specification. Prerequisites In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to change the network parameters. We have already prepared several benchmarks to use. If you want to create your own benchmark specification, check out this guide . Step 1 - Inject latency for each source endpoint KOBE allows simulating network traffic for all sources of the benchmark. For every source dataset of the benchmark, you can: inject delay in the connection between the given source endpoint and the federation engine . inject delay in the connection between the given source endpoint and another source endpoint . The reason for injecting delays between the federated sources is the fact that every SPARQL endpoint can issue a SPARQL query to every other endpoint using the SERVICE SPARQL keyword. The latency of each source can be configured using the following delay parameters . The functionality of these parameters is offered by Istio. Check this link for more information. The fixedDelaySec and fixedDelayMSec are used to indicate the amount of delay in seconds and in milliseconds. The percentage field can be used to only delay a certain percentage of requests . You can extend your benchmark specification can be extended in order to define the latency of the sources as follows: # In this example we will use two datasets, ds1 and ds2. spec : datasets : - name : ds1 # adds 1 second of delay before forwarding all responces to the federator federatorConnection : delayInjection : fixedDelaySec : 1 percentage : 100 networkTopology : # adds 2 sec of delay before forwarding the 50% of responces to the source ds1 - datasetSource : ds2 delayInjection : fixedDelaySec : 2 percentage : 50 # ... add remaining parameters for ds1 - name : ds2 # ... add remaining parameters for ds2 Check the following link in which we illustrate a simple working example with delays: benchmark-toybench/toybench-delays.yaml This benchmark contains three SPARQL queries and two datasets (namely toy1 and toy2 ). All responses from toy1 to the federator are delayed by 2 seconds and 150 milliseconds, all responses from toy2 to the federator are delayed by 2 seconds, and the 50% of the responses from toy1 to toy2 are delayed by 3 seconds. Examples We have already prepared a benchmark specification with delays to experiment with: benchmark-toybench Note We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Tune network settings"},{"location":"use/tune_network/#tune-network-settings","text":"This walkthrough illustrates the steps required from the benchmark designer in order to configure the latency of the data sources of a Benchmark specification.","title":"Tune network settings"},{"location":"use/tune_network/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to change the network parameters. We have already prepared several benchmarks to use. If you want to create your own benchmark specification, check out this guide .","title":"Prerequisites"},{"location":"use/tune_network/#step-1-inject-latency-for-each-source-endpoint","text":"KOBE allows simulating network traffic for all sources of the benchmark. For every source dataset of the benchmark, you can: inject delay in the connection between the given source endpoint and the federation engine . inject delay in the connection between the given source endpoint and another source endpoint . The reason for injecting delays between the federated sources is the fact that every SPARQL endpoint can issue a SPARQL query to every other endpoint using the SERVICE SPARQL keyword. The latency of each source can be configured using the following delay parameters . The functionality of these parameters is offered by Istio. Check this link for more information. The fixedDelaySec and fixedDelayMSec are used to indicate the amount of delay in seconds and in milliseconds. The percentage field can be used to only delay a certain percentage of requests . You can extend your benchmark specification can be extended in order to define the latency of the sources as follows: # In this example we will use two datasets, ds1 and ds2. spec : datasets : - name : ds1 # adds 1 second of delay before forwarding all responces to the federator federatorConnection : delayInjection : fixedDelaySec : 1 percentage : 100 networkTopology : # adds 2 sec of delay before forwarding the 50% of responces to the source ds1 - datasetSource : ds2 delayInjection : fixedDelaySec : 2 percentage : 50 # ... add remaining parameters for ds1 - name : ds2 # ... add remaining parameters for ds2 Check the following link in which we illustrate a simple working example with delays: benchmark-toybench/toybench-delays.yaml This benchmark contains three SPARQL queries and two datasets (namely toy1 and toy2 ). All responses from toy1 to the federator are delayed by 2 seconds and 150 milliseconds, all responses from toy2 to the federator are delayed by 2 seconds, and the 50% of the responses from toy1 to toy2 are delayed by 3 seconds.","title":"Step 1 - Inject latency for each source endpoint"},{"location":"use/tune_network/#examples","text":"We have already prepared a benchmark specification with delays to experiment with: benchmark-toybench Note We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Examples"}]}